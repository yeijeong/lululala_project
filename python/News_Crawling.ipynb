{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4.element\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(\n",
    "    data=None\n",
    "    , index=None\n",
    "    , columns=['date','title','link']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConnectionError방지\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "def get_soup_obj(url):\n",
    "    res = requests.get(url, headers = headers, verify=False)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(url, headers=headers, verify=False)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 기본 정보 가져오기\n",
    "def get_top3_news_info():\n",
    "    news_urls =[]\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 목록 주소\n",
    "    sec_url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001\"\n",
    "    \n",
    "    # 해당 분야 상위 뉴스 HTML 가져오기\n",
    "    soup = get_soup_obj(sec_url)\n",
    "  \n",
    "    # 해당 분야 상위 뉴스 3개 가져오기\n",
    "    lis3 = soup.find('ul', class_='type06_headline').find_all(\"li\", limit=3)\n",
    "    for li in lis3:\n",
    "        news_url = li.a.attrs.get('href')\n",
    "        news_urls.append(news_url)\n",
    "        \n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 3개 뉴스 크롤링\n",
    "def F_crawling(news_urls) :\n",
    "    news_titles = []\n",
    "    news_dates = []\n",
    "    news_url = []\n",
    "\n",
    "    for url in news_urls:\n",
    "        news_html = get_soup_obj(url)\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "\n",
    "        news_titles.append(title)\n",
    "\n",
    "        # 날짜 가져오기\n",
    "        try:\n",
    "            html_date = news_html.select_one(\"div#ct > div.media_end_head > div.media_end_head_info > div span\")\n",
    "            news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "            news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "        news_dates.append(news_date)\n",
    "\n",
    "        news_url.append(url)\n",
    "    \n",
    "    a = pd.DataFrame({'date':news_dates,'title':news_titles,'link':news_url})\n",
    "\n",
    "    #중복 행 지우기\n",
    "    a = a.drop_duplicates(keep='first',ignore_index=True)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(d):\n",
    "  text = re.sub(r'\\([^)]*\\)', '', str(d))\n",
    "  text = re.sub(r'\\[[^]]*\\]', '', text)\n",
    "  text = re.sub(r'\\<[^>]*\\>', '', text)\n",
    "  pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  text = re.sub(r'사진', ' ', text)\n",
    "  text = re.sub(r'.*뉴스', ' ', text)\n",
    "  text = re.sub(\"\\n\", ' ', text)\n",
    "  text = re.sub(\"  +\", \" \", text)\n",
    "  return text\n",
    "\n",
    "def text_clean(text):\n",
    "  pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\n]'            # \\n제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\t]'            # \\n제거\n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\']'           \n",
    "  text = re.sub(pattern, '', text)\n",
    "  pattern = '[\\\"]'            \n",
    "  text = re.sub(pattern, '', text)\n",
    "  return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dbeaver(NewsDate, NewsTitle, NewsLink) :\n",
    "    conn = pymysql.connect(\n",
    "        host='localhost'\n",
    "        , user='root'\n",
    "        , password='1234'\n",
    "        , db='lululala_project'\n",
    "        , charset='utf8'\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    for date,title,link in zip(NewsDate, NewsTitle, NewsLink):\n",
    "        sql = \"INSERT IGNORE INTO news_crawling (NewsDate, NewsTitle, NewsLink) VALUES ({}, {}, {})\".format(\"\\\"\"+date+\"\\\"\", \"\\\"\"+title+\"\\\"\", \"\\\"\"+link+\"\\\"\")\n",
    "        try : \n",
    "            cur.execute(sql)\n",
    "        except Exception as e :\n",
    "            print('Error Message :', e)\n",
    "            return\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5년간 광주 전남 뺑소니 사고 운전자 면허취소 1591건\n",
      "1          육참총장 인도 방문 군사외교활동 방산협력 확대\n",
      "2     강서구청장 선거 공방 여 방탄용 공천 야 숙원사업 협박\n",
      "Name: title, dtype: object\n",
      "Series([], Name: title, dtype: object)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m NewsLink \u001b[39m=\u001b[39m df_all2[\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m     25\u001b[0m to_dbeaver(NewsDate, NewsTitle, NewsLink)\n\u001b[0;32m---> 27\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 무한루프 크롤링\n",
    "while True :\n",
    "    news_urls = get_top3_news_info()\n",
    "    a = F_crawling(news_urls)\n",
    "    df_all = pd.merge(a, news_df, how='outer', indicator=True)\n",
    "    news_df = pd.concat([news_df, a], ignore_index=True, keys=['date','title','link'])\n",
    "    news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "    str_expr = '_merge == \\\"left_only\\\"'\n",
    "    df_all2 = df_all.query(str_expr)\n",
    "    df_all2.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "    df_all2.dropna(axis=0, inplace=True)\n",
    "\n",
    "    df_all2['title'] = df_all2['title'].apply(text_clean)\n",
    "    df_all2['title'] = df_all2['title'].apply(clean_text)\n",
    "\n",
    "    df_all2.dropna(axis=0, inplace=True)\n",
    "\n",
    "    print(df_all2['title'])\n",
    "\n",
    "    NewsDate = df_all2['date'].to_list()\n",
    "    NewsTitle = df_all2['title'].to_list()\n",
    "    NewsLink = df_all2['link'].to_list()\n",
    "\n",
    "    to_dbeaver(NewsDate, NewsTitle, NewsLink)\n",
    "                        \n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
